{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>CONVOLUTIONAL NEURAL NETWORKS | ML FINAL PROJECT</center></h1>\n",
    "Contains the model architecture, training, testing, and visualization code for the two neural networks I coded from scratch for this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "######   LIBRARIES   ######\n",
    "###########################\n",
    "\n",
    "# ------ STANDARD ------ #\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "%matplotlib inline\n",
    "\n",
    "# ------ TORCH MODULES ------ #\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim import lr_scheduler\n",
    "from torch_lr_finder import LRFinder\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "# ------ SKLEARN MODULES ------ #\n",
    "import scipy.io\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "###### IMAGE DISPLAY ######\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- PATH -----\n",
    "training_directory = 'C:\\\\Users\\\\17725\\\\dataScienceNCF\\\\Spring2020_MLProject\\\\data\\\\train'\n",
    "validation_directory = 'C:\\\\Users\\\\17725\\\\dataScienceNCF\\\\Spring2020_MLProject\\\\data\\\\validation'\n",
    "test_directory = 'C:\\\\Users\\\\17725\\\\dataScienceNCF\\\\Spring2020_MLProject\\\\data\\\\test'\n",
    "\n",
    "\n",
    "# ----- TRANSFORMS / STANDARDIZATION -----\n",
    "\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "standard_transform = tt.Compose([\n",
    "                        tt.Resize((150, 150)),\n",
    "                        tt.RandomHorizontalFlip(p = 0.5),\n",
    "                        tt.ToTensor(), \n",
    "                        tt.Normalize(mean = imagenet_mean, \n",
    "                                  std = imagenet_std)\n",
    "                    ])\n",
    "\n",
    "# ----- BUILD IMAGE FOLDERS -----\n",
    "\n",
    "training_data = ImageFolder(training_directory, \n",
    "                            transform = standard_transform)\n",
    "\n",
    "validation_data = ImageFolder(validation_directory, \n",
    "                              transform = standard_transform)\n",
    "\n",
    "test_data = ImageFolder(test_directory, \n",
    "                              transform = standard_transform)\n",
    "\n",
    "# ----- DATA LOADERS -----\n",
    "\n",
    "# Batch size \n",
    "batch_size = 10\n",
    "\n",
    "train_loader = DataLoader(training_data, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=4, \n",
    "                          pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(validation_data, \n",
    "                        batch_size*2, \n",
    "                        num_workers=4, \n",
    "                        pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(test_data, \n",
    "                        batch_size*2, \n",
    "                        num_workers=4, \n",
    "                        pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>HELPER FUNCTIONS</b></center>\n",
    "Functions provided in the Jovian lectures that expedite the training process. The hyyperparameters and data are passed into the fit function, which proceeds to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "\n",
    "#   ----- MOVE TO DEVICE -----  #\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "#   ----- DATA LOADER WRAPPER  -----  #\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "#   ----- EVALULATE -----  #\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "#  ----- FIT THE MODEL  -----  #\n",
    "\n",
    "def fit(epochs, model, train_loader, val_loader, optimizer, name):\n",
    "    history = []\n",
    "    cur_accurary = 0\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # -- Training -- #\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        for batch in train_loader:\n",
    "            model_out = model.training_step(batch)\n",
    "            loss = model_out['loss']\n",
    "            train_losses.append(loss)\n",
    "            train_accuracies.append(model_out['acc'])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        # -- Validation -- #\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['train_acc'] = torch.stack(train_accuracies).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        cur_accuracy = result['val_acc']\n",
    "        if cur_accuracy > best_accuracy:\n",
    "                torch.save(model.state_dict(), '{0}'.format(name))\n",
    "                best_accuracy = cur_accuracy\n",
    "        history.append(result)\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>IMAGE CLASSIFICATION BASE</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \n",
    "    '''\n",
    "    Abstract class that extends nn.Module and adds some functions that make training a lot nicer. \n",
    "    The functions provide the steps taken when training and validating the model, as well as code \n",
    "    to print the loss and accuracy at each epoch. \n",
    "    '''\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  \n",
    "        loss = F.cross_entropy(out, labels) \n",
    "        acc = accuracy(out, labels)\n",
    "        return {'loss': loss, 'acc': acc}\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    \n",
    "        loss = F.cross_entropy(out, labels)   \n",
    "        acc = accuracy(out, labels)           \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, train_acc: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['train_acc'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>CLASS OBJECT FOR TINYNET</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyNet(ImageClassificationBase):\n",
    "    \n",
    "    '''\n",
    "    Convolutional neural network created to classify cat breeds. Named TinyNet because it\n",
    "    is not very large.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "           \n",
    "            # ----- LAYER 1 ----- #\n",
    "            # input: 3 x 150 x 150\n",
    "            nn.Conv2d(3, 30, kernel_size = 5, padding = 5//2),\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.MaxPool2d((3,3), stride =  2),\n",
    "            \n",
    "            # ----- LAYER 2 ----- #\n",
    "            nn.Conv2d(30, 30, kernel_size = 5, padding = 5//2),\n",
    "            nn.BatchNorm2d(30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d((3,3), stride =  2),\n",
    "    \n",
    "            # ----- CLASSIFIER----- #\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(38880, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 15))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>CLASS OBJECT FOR BABYNET</h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class BabyNet(ImageClassificationBase):\n",
    "    '''\n",
    "    Convolutional neural network created to classify cat breeds. Named BabyNet due to it's \n",
    "    accuracy as being no better than how I expect a baby would perform whehn asked to classify\n",
    "    cat breeds. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "           \n",
    "            # ----- BLOCK 1 ----- #\n",
    "            # input: 3 x 150 x 150\n",
    "            nn.Conv2d(3, 16, kernel_size = 5, padding = 5//2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.8),\n",
    "            nn.MaxPool2d((3,3), stride =  2),\n",
    "            # output: 50 x 74 x 74\n",
    "            \n",
    "            # ----- BLOCK 2 ----- #\n",
    "            nn.Conv2d(16, 36, kernel_size = 5, padding = 5//2),\n",
    "            nn.BatchNorm2d(36),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d((3,3), stride =  2),\n",
    "    \n",
    "            # ----- BLOCK 3 ----- #\n",
    "            nn.Conv2d(36, 72, kernel_size = 5, padding = 5//2),\n",
    "            nn.BatchNorm2d(72),\n",
    "            nn.Conv2d(72, 100, kernel_size = 5, padding = 5//2),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.Conv2d(100, 75, kernel_size = 5, padding = 5//2),\n",
    "            nn.BatchNorm2d(75),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.MaxPool2d((3,3), stride =  2),\n",
    "            nn.AvgPool2d((3,3), stride =  2),\n",
    "    \n",
    "            # ----- CLASSIFIER----- #\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4800, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 15))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>BABYNET MODEL</h2></center>\n",
    "01 / Convolutional neural network with five convolutional layers and two linear layers. \n",
    "  \n",
    "02 / No softmax function because softmax is implemented within cross-entropy loss and the Pytorch documentation specifically states not to use softmax as the final activation when also using cross-entropy loss.     \n",
    "  \n",
    "03 / Performs badly, but better than the null model (which would be about 7% accuracy)\n",
    "\n",
    "04 / References the AlexNet architecture (https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) for the blocking design. I decided to use AlexNet as a base because it is a relatively shallow design (small dataset --> deeper architecture would be more prone to over-fitting and is not appropriate) and is a classic that I wanted to understand better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BabyNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(BabyNet().parameters(), lr = 1e-7, weight_decay = 0.001)\n",
    "\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device = \"cuda\")\n",
    "lr_finder.range_test(train_loader, end_lr = 1, num_iter = 100)\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ HYPERPARAMETERS ------ #\n",
    "# Adjustments to the hyperparameters made VERY little difference in overall model performance\n",
    "\n",
    "# Optimizers tried: \n",
    "#      Adam and SGD ✔️\n",
    "# Learning Rate:\n",
    "#      Determined by the LR finder; tried several iterations, little difference in results\n",
    "# Weight Decay Tried:\n",
    "#      0.0001, 0.0003, 0.001 ✔️, 0.003, 0.01\n",
    "# Kernel Sizes Tried:\n",
    "#      3x3, 5x5✔️, 7x7\n",
    "# Epochs trained for: \n",
    "#      Up to 150; no additional benefit to validation accuracy gained from letting the model\n",
    "#      train beyond ~50 epochs\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = to_device(BabyNet(), device)\n",
    "epochs = 50\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.002, weight_decay = 0.001)\n",
    "\n",
    "# *** fit the model here ***\n",
    "train_dl = DeviceDataLoader(train_loader, device)\n",
    "val_dl = DeviceDataLoader(val_loader, device)\n",
    "\n",
    "history = fit(epochs, model, train_dl, val_dl, optimizer, 'bestBabyNet.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TEST BABYNET</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Load in the Best BabyNet ------ #\n",
    "\n",
    "best_BabyNet = BabyNet()\n",
    "best_BabyNet.load_state_dict(torch.load('bestBabyNet.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Predict ------ #\n",
    "\n",
    "def predict(model, images):\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted\n",
    "\n",
    "acc_list = []\n",
    "for batch in test_loader:\n",
    "    images, labels = batch\n",
    "    preds = predict(best_BabyNet, images)\n",
    "    test_accuracy = torch.tensor(torch.sum(preds == labels).item() / len(preds)).item()\n",
    "    acc_list.append(test_accuracy)\n",
    "    \n",
    "statistics.mean(acc_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><center>Accuracy/Loss Visualization</center></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_values = []\n",
    "val_acc_values = []\n",
    "train_loss_values = []\n",
    "train_acc_values = []\n",
    "\n",
    "for i in range(50):\n",
    "    val_loss_values.append((history[i]['val_loss']))\n",
    "    val_acc_values.append((history[i]['val_acc']))\n",
    "    train_loss_values.append((history[i]['train_loss']))\n",
    "    train_acc_values.append((history[i]['train_acc']))\n",
    "\n",
    "epochs = np.linspace(0,50, num = 50)\n",
    "\n",
    "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "plt.plot(epochs, val_acc_values)\n",
    "plt.plot(epochs, train_acc_values)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2) # index 2\n",
    "plt.plot(epochs, val_loss_values)\n",
    "plt.plot(epochs, train_loss_values)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>TINYNET MODEL</h2></center>\n",
    "01 / Convolutional neural network with two convolutional layers, two max-pools, and two linear layers. Combines batch normalization and drop-out in an attempt to avoid over-fitting. According to the relevant literature, regularization is not typically necessary for shallow neural networks. However, the model did no better than chance without regularization and as such I decided to try it (because it couldn't hurt).  \n",
    "  \n",
    "02 / No softmax function because softmax is implemented within cross-entropy loss and the Pytorch documentation specifically states not to use softmax as the final activation when also using cross-entropy loss.     \n",
    "  \n",
    "03 / Performs badly, but better than the null model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(TinyNet().parameters(), lr = 1e-7, weight_decay = 0.001)\n",
    "\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device = \"cuda\")\n",
    "lr_finder.range_test(train_loader, end_lr = 1, num_iter = 100)\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### HYPERPARAMETERS #####\n",
    "device = torch.device('cuda')\n",
    "model = to_device(TinyNet(), device)\n",
    "epochs = 50\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.003)\n",
    "\n",
    "# *** fit the model here ***\n",
    "train_dl = DeviceDataLoader(train_loader, device)\n",
    "val_dl = DeviceDataLoader(val_loader, device)\n",
    "\n",
    "history = fit(epochs, model, train_dl, val_dl, optimizer, 'bestTinyNet.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_values = []\n",
    "val_acc_values = []\n",
    "train_loss_values = []\n",
    "train_acc_values = []\n",
    "\n",
    "for i in range(50):\n",
    "    val_loss_values.append((history[i]['val_loss']))\n",
    "    val_acc_values.append((history[i]['val_acc']))\n",
    "    train_loss_values.append((history[i]['train_loss']))\n",
    "    train_acc_values.append((history[i]['train_acc']))\n",
    "    \n",
    "epochs = np.linspace(0,50)\n",
    "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "plt.plot(epochs, val_acc_values)\n",
    "plt.plot(epochs, train_acc_values)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2) # index 2\n",
    "plt.plot(epochs, val_loss_values)\n",
    "plt.plot(epochs, train_loss_values)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Load in the Best TinyNet ------ #\n",
    "\n",
    "best_TinyNet = TinyNet()\n",
    "best_TinyNet.load_state_dict(torch.load('bestTinyNet.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------ Predict ------ #\n",
    "\n",
    "def predict(model, images):\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted\n",
    "\n",
    "acc_list = []\n",
    "for batch in test_loader:\n",
    "    images, labels = batch\n",
    "    preds = predict(best_TinyNet, images)\n",
    "    test_accuracy = torch.tensor(torch.sum(preds == labels).item() / len(preds)).item()\n",
    "    acc_list.append(test_accuracy)\n",
    "    \n",
    "statistics.mean(acc_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><center>Accuracy/Loss Visualization</center></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_values = []\n",
    "val_acc_values = []\n",
    "train_loss_values = []\n",
    "train_acc_values = []\n",
    "\n",
    "for i in range(50):\n",
    "    val_loss_values.append((history[i]['val_loss']))\n",
    "    val_acc_values.append((history[i]['val_acc']))\n",
    "    train_loss_values.append((history[i]['train_loss']))\n",
    "    train_acc_values.append((history[i]['train_acc']))\n",
    "\n",
    "epochs = np.linspace(0,50, num = 50)\n",
    "\n",
    "plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
    "plt.plot(epochs, val_acc_values)\n",
    "plt.plot(epochs, train_acc_values)\n",
    "plt.title(\"Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2) # index 2\n",
    "plt.plot(epochs, val_loss_values)\n",
    "plt.plot(epochs, train_loss_values)\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
